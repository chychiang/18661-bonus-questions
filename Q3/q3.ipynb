{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'lib')\n",
    "\n",
    "import lib\n",
    "import time, argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lib.common_utils import TabularUtils\n",
    "from lib.regEnvs import *\n",
    "\n",
    "class Tabular_DP:\n",
    "    def __init__(self, args):\n",
    "        self.env = args.env\n",
    "        self.gamma = 0.99 # discount factor\n",
    "        self.theta = 1e-5 # small threshold for stopping criteria\n",
    "        self.max_iterations = 1000\n",
    "        self.num_actions = self.env.action_space.n # number of actions\n",
    "        self.num_states = self.env.observation_space.n # number of states\n",
    "\n",
    "    def compute_q_value_current_state(self, state, value_function):\n",
    "        q_values = np.zeros(self.num_actions)\n",
    "        for action in range(self.num_actions):\n",
    "            for prob, next_state, reward, _ in self.env.P[state][action]:\n",
    "                q_values[action] += prob * (reward + self.gamma * value_function[next_state])\n",
    "        return q_values\n",
    "\n",
    "    def action_to_one_hot(self, action):\n",
    "        one_hot_action = np.zeros(self.num_actions)\n",
    "        one_hot_action[action] = 1\n",
    "        return one_hot_action\n",
    "\n",
    "    def value_iteration(self):\n",
    "        value_function = np.zeros(self.num_states)\n",
    "        optimal_policy = np.zeros((self.num_states, self.num_actions))\n",
    "        for _ in range(self.max_iterations):\n",
    "            delta = 0\n",
    "            for state in range(self.num_states):\n",
    "                q_values = self.compute_q_value_current_state(state, value_function)\n",
    "                best_action_value = np.max(q_values)\n",
    "                delta = max(delta, np.abs(best_action_value - value_function[state]))\n",
    "                value_function[state] = best_action_value\n",
    "                optimal_policy[state] = self.action_to_one_hot(np.argmax(q_values))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return value_function, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Running value iteration=====================\n",
      "Optimal value function: \n",
      "[0.46089055 0.56655611 0.673289   0.56655611 0.56655611 0.\n",
      " 0.7811     0.         0.673289   0.7811     0.89       0.\n",
      " 0.         0.89       1.         0.        ]\n",
      "Optimal policy: \n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'FrozenLake-Deterministic-v1'\n",
    "# env_name = 'FrozenLake-Deterministic-8x8-v1'\n",
    "\n",
    "\n",
    "my_env = gym.make(env_name)\n",
    "tabularUtils = TabularUtils(my_env)\n",
    "# # test value iteration\n",
    "dp = Tabular_DP(my_env)\n",
    "print(\"================Running value iteration=====================\")\n",
    "V_optimal, policy_optimal = dp.value_iteration()\n",
    "print(\"Optimal value function: \")\n",
    "print(V_optimal)\n",
    "print(\"Optimal policy: \")\n",
    "print(policy_optimal)\n",
    "# render\n",
    "# tabularUtils.render(policy_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tabular_TD:\n",
    "    def __init__(self, env):\n",
    "        # Initialize the Tabular_TD class with the given environment\n",
    "        self.env = env\n",
    "        self.num_episodes = 10000  # Number of episodes for training\n",
    "        self.gamma = 0.99  # Discount factor for future rewards\n",
    "        self.alpha = 0.05  # Learning rate\n",
    "        self.env_nA = self.env.action_space.n  # Number of actions in the environment\n",
    "        self.env_nS = self.env.observation_space.n  # Number of states in the environment\n",
    "        self.tabular_utils = TabularUtils(self.env)  # Utility functions for the tabular environment\n",
    "\n",
    "    def sarsa(self):\n",
    "        \"\"\"sarsa: on-policy TD control\"\"\"\n",
    "        # Initialize Q values with zeros\n",
    "        Q_values = np.zeros((self.tabular_utils.env_nS, self.tabular_utils.env_nA))\n",
    "        epsilon = 0.1  # Epsilon value for epsilon-greedy policy\n",
    "        for epi in range(self.num_episodes):\n",
    "            current_state = self.env.reset()  # Reset the environment to the initial state\n",
    "            current_action = self.tabular_utils.epsilon_greedy_policy(Q_values[current_state], epsilon)\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Execute the action in the environment\n",
    "                next_state, reward, done, _ = self.env.step(current_action)\n",
    "                next_action = self.tabular_utils.epsilon_greedy_policy(Q_values[next_state], epsilon)\n",
    "                # SARSA update\n",
    "                td_target = reward + (self.gamma * Q_values[next_state][next_action] * (not done))\n",
    "                td_error = td_target - Q_values[current_state][current_action]\n",
    "                Q_values[current_state][current_action] += self.alpha * td_error\n",
    "                current_state, current_action = next_state, next_action\n",
    "\n",
    "        # Derive the greedy policy from the Q values\n",
    "        greedy_policy = self.tabular_utils.Q_value_to_greedy_policy(Q_values)\n",
    "        return Q_values, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy from SARSA\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def action_to_one_hot(action, num_actions=4):\n",
    "    # Convert the given action to a one-hot encoded vector\n",
    "    one_hot_action = np.zeros(num_actions)\n",
    "    one_hot_action[action] = 1\n",
    "    return one_hot_action\n",
    "\n",
    "my_env = gym.make(env_name)\n",
    "tabularUtils = TabularUtils(my_env)\n",
    "\n",
    "# Create an instance of the Tabular_TD class and test SARSA\n",
    "td = Tabular_TD(my_env)\n",
    "Q_sarsa, policy_sarsa = td.sarsa()\n",
    "print(\"Policy from SARSA\")\n",
    "# Convert the policy to a deterministic policy and represent it as a list of one-hot vectors\n",
    "best_policy = tabularUtils.onehot_policy_to_deterministic_policy(policy_sarsa)\n",
    "best_policy = [action_to_one_hot(int(i)) for i in best_policy]\n",
    "\n",
    "\n",
    "print(np.array(best_policy))\n",
    "# Visualize the best policy on the environment\n",
    "# tabularUtils.render(best_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
