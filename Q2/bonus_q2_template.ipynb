{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13085852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"fashion-mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f021a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from data import get_data_loader\n",
    "# from network import Network\n",
    "\n",
    "try:\n",
    "    from termcolor import cprint\n",
    "except ImportError:\n",
    "    cprint = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'lr': 0.01,\n",
    "    'weight_decay': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'nesterov': True,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'log_every': 1,\n",
    "    'val_every': 1,\n",
    "    'num_workers': 1,\n",
    "    'patience': 5,\n",
    "    'lr_decay': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNIST(Dataset):\n",
    "    def __init__(self, set_name, data_augmentation=True):\n",
    "        super(FMNIST, self).__init__()\n",
    "        # TODO: Retrieve all the images and the labels, and store them\n",
    "        # as class variables. Maintaing any other class variables that \n",
    "        # you might need for the other class methods. Note that the \n",
    "        # methods depends on the set (train or test) and thus maintaining\n",
    "        # that is essential. Consider data_augmentation=True and False cases\n",
    "\n",
    "        DATA_PATH = \"fashion-mnist\"\n",
    "        set_name = set_name.lower()\n",
    "        if set_name not in ['train', 'test']:\n",
    "            # raise\n",
    "\n",
    "        else:\n",
    "            #if set_name == 'train':\n",
    "                #self.labels_file =  \n",
    "                #self.images_file =  \n",
    "\n",
    "            #elif set_name == 'test':\n",
    "                #self.labels_file =  \n",
    "                #self.images_file =  \n",
    "\n",
    "            #images_path =\n",
    "            #labels_path = \n",
    "\n",
    "\n",
    "            if data_augmentation is True and set_name=='train':\n",
    "                #TODO\n",
    "\n",
    "            else:\n",
    "                #self.images = \n",
    "                #self.labels = \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Complete this\n",
    "        #return \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # TODO: Complete this\n",
    "        #labels = \n",
    "        #images = \n",
    "        #return \n",
    "\n",
    "def normalize_data(input_data):\n",
    "    n = input_data.shape[0]\n",
    "    r = input_data.shape[1]\n",
    "    c = input_data.shape[2]\n",
    "    input_data = input_data.reshape(n, r*c)\n",
    "    normalized_input_data = (input_data - input_data.min(axis=1, keepdims=True)) / (input_data.max(axis=1, keepdims=True) - input_data.min(axis=1, keepdims=True))\n",
    "    normalized_input_data = normalized_input_data.reshape((n, r, c))\n",
    "    return normalized_input_data\n",
    "\n",
    "def get_data_loader(set_name, data_augmentation=False):\n",
    "    # TODO: Create the dataset class tailored to the set (train or test)\n",
    "    # provided as argument. Use it to create a dataloader. Use the appropriate\n",
    "    # hyper-parameters from cfg\n",
    "    #dataset = \n",
    "    #data_loader = \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff664250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(param):\n",
    "    # NOTE: Not for Vanilla Classifier\n",
    "    # TODO: Complete this to initialize the weights\n",
    "    if isinstance(param, nn.Linear):\n",
    "        #nn.\n",
    "        #nn.\n",
    "\n",
    "def zero_init(param):\n",
    "    # NOTE: Not for Vanilla Classifier\n",
    "    # TODO: Complete this to initialize the weights\n",
    "    if isinstance(param, nn.Linear):\n",
    "        #nn.\n",
    "        #nn.\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, dropout_strategy=False):\n",
    "        super(Network, self).__init__()\n",
    "        # TODO: Define the model architecture here\n",
    "        # Define proportion or neurons to dropout\n",
    "        #self.dropout_strategy = \n",
    "        #self.flatten = \n",
    "\n",
    "        if self.dropout_strategy:\n",
    "            #TODO\n",
    "        else:\n",
    "            #TODO\n",
    "\n",
    "        # Dummy initialization of optimizer variable useful to load the optimizer\n",
    "        # It is truly initialized before saving in train.py's train function\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=0)\n",
    "\n",
    "        # NOTE: Not for Vanilla Classsifier\n",
    "        # TODO: Initalize weights by calling the\n",
    "        # init_weights method\n",
    "        #self.\n",
    "\n",
    "    def init_weights(self, init_weights_strategy = None):\n",
    "        # NOTE: Not for Vanilla Classsifier\n",
    "        # TODO: Initalize weights by calling by using the\n",
    "        # appropriate initialization function\n",
    "        #self.init_weights_strategy = \n",
    "        if (init_weights_strategy is None) or (init_weights_strategy == \"vanilla\") :\n",
    "            #TODO (Hint: This is Vanilla Classifier Default)\n",
    "\n",
    "        elif init_weights_strategy == \"zero\":\n",
    "            #self.\n",
    "        elif init_weights_strategy == \"xavier\":\n",
    "            #self.\n",
    "        else:\n",
    "            print(\"No '{}' initialization strategy available \".format(init_weights_strategy))\n",
    "            print(\"Available Strategies: \\n'xavier'\\n'zero'\")\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Define the forward function of your model\n",
    "        #x = \n",
    "        #logits = \n",
    "        #return \n",
    "    \n",
    "    def save(self, ckpt_path):\n",
    "        # TODO: Save the checkpoint of the model\n",
    "        #torch.\n",
    "\n",
    "    def load(self, ckpt_path):\n",
    "        # TODO: Load the checkpoint of the model\n",
    "        #checkpoint = \n",
    "        #self.load_state_dict(#TODO)\n",
    "        #self.optimizer.load_state_dict(#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2664f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def log_print(text, color=None, on_color=None, attrs=None):\n",
    "    if cprint is not None:\n",
    "        cprint(text, color=color, on_color=on_color, attrs=attrs)\n",
    "    else:\n",
    "        print(text)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    #TODO: Returns the current Learning Rate being used by\n",
    "    # the optimizer\n",
    "    for param_group in optimizer.param_groups:\n",
    "        #return \n",
    "\n",
    "'''\n",
    "Use the average meter to keep track of average of the loss or \n",
    "the test accuracy! Just call the update function, providing the\n",
    "quantities being added, and the counts being added\n",
    "'''\n",
    "class AvgMeter():\n",
    "    def __init__(self):\n",
    "        self.qty = 0\n",
    "        self.cnt = 0\n",
    "    \n",
    "    def update(self, increment, count):\n",
    "        self.qty += increment\n",
    "        self.cnt += count\n",
    "    \n",
    "    def get_avg(self):\n",
    "        if self.cnt == 0:\n",
    "            return 0\n",
    "        else: \n",
    "            return self.qty/self.cnt\n",
    "\n",
    "\n",
    "def run(net, epoch, loader, optimizer, criterion, logger, scheduler, train=True):\n",
    "    # TODO: Initalize the different Avg Meters for tracking loss and accuracy (if test)\n",
    "    avg_loss = AvgMeter()\n",
    "    if train is False:\n",
    "        avg_accuracy = AvgMeter()\n",
    "\n",
    "    # TODO: Performs a pass over data in the provided loader\n",
    "    # TODO: Iterate over the loader and find the loss. Calculate the loss and based on which\n",
    "    # set is being provided update you model. Also keep track of the accuracy if we are running\n",
    "    # on the test set.\n",
    "    for batch, (X, y) in enumerate(loader):\n",
    "        # Compute prediction and loss\n",
    "\n",
    "        if train is True:\n",
    "            # Backpropagation\n",
    "            \n",
    "        else:\n",
    "            #accuracy = \n",
    "            #avg_accuracy.\n",
    "\n",
    "    # TODO: Log the training/testing loss using tensorboard.\n",
    "    if train is True:\n",
    "        #logger.\n",
    "    else:\n",
    "        #logger.\n",
    "\n",
    "    # TODO: return the average loss, and the accuracy (if test set)\n",
    "    if train is False:\n",
    "        #return \n",
    "    else:\n",
    "        #return \n",
    "        \n",
    "\n",
    "def train(net, train_loader, test_loader, logger):\n",
    "    # TODO: Define the SGD optimizer here. Use hyper-parameters from cfg\n",
    "    #optimizer = \n",
    "    #net.optimizer =\n",
    "    # TODO: Define the criterion (Objective Function) that you will be using\n",
    "    #criterion = \n",
    "    # TODO: Define the ReduceLROnPlateau scheduler for annealing the learning rate\n",
    "    #scheduler = \n",
    "\n",
    "    for i in range(cfg['epochs']):\n",
    "        # Run the network on the entire train dataset. Return the average train loss\n",
    "        # Note that we don't have to calculate the accuracy on the train set.\n",
    "        loss, _ = run(net, i, train_loader, optimizer, criterion, logger, scheduler)\n",
    "\n",
    "        # TODO: Get the current learning rate by calling get_lr() and log it to tensorboard\n",
    "        #current_learning_rate = get_lr(optimizer)\n",
    "        #logger.\n",
    "        \n",
    "        # Logs the training loss on the screen, while training\n",
    "        if i % cfg['log_every'] == 0:\n",
    "            log_text = \"Epoch: [%d/%d], Training Loss:%2f\" % (i, cfg['epochs'], loss)\n",
    "            log_print(log_text, color='green', attrs=['bold'])\n",
    "\n",
    "        # Evaluate our model and add visualizations on tensorboard\n",
    "        if i % cfg['val_every'] == 0:\n",
    "            # TODO: HINT - you might need to perform some step before and after running the network\n",
    "            # on the test set\n",
    "            # Run the network on the test set, and get the loss and accuracy on the test set\n",
    "\n",
    "\n",
    "            # TODO: Perform a step on the scheduler, while using the Accuracy on the test set\n",
    "            #scheduler.step(metrics=acc)\n",
    "            \n",
    "            # TODO: Use tensorboard to log the Test Accuracy and also to perform visualization of the\n",
    "            #logger.\n",
    "            # 2 weights of the first layer of the network!\n",
    "            img = net.linear_relu_stack[0].weight[0].reshape(28, 28)\n",
    "            img = np.reshape(img.detach().numpy(), (28, 28))\n",
    "            logger.add_image('Image of weights connecting all the input nodes to first node of the first hidden layer for epoch {}: '.format(i), img,dataformats='HW')\n",
    "\n",
    "            img = net.linear_relu_stack[0].weight[1].reshape(28, 28)\n",
    "            img = np.reshape(img.detach().numpy(), (28, 28))\n",
    "            logger.add_image('Image of weights connecting all the input nodes to second node of the first hidden layer for epoch {}: '.format(i), img, dataformats='HW')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d905d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_strategy = False\n",
    "data_augmentation = False\n",
    "\n",
    "#3 options -> 'vanilla' or 'zero' or 'xavier'\n",
    "init_weights_strategy = 'vanilla'\n",
    "\n",
    "# TODO: Create a network object and initialize\n",
    "#net = \n",
    "#net.init_weights()\n",
    "\n",
    "# TODO: Create a tensorboard object for logging\n",
    "#writer = \n",
    "\n",
    "# TODO: Create train data loader\n",
    "#train_loader \n",
    "\n",
    "# TODO: Create test data loader\n",
    "#test_loader = \n",
    "\n",
    "# Run the training!\n",
    "train(net, train_loader, test_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
